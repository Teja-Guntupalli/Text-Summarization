{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training a Summarization Model\n\nNow let's see how we can use `HuggingFace` to train a summarization model on a new dataset. We'll use the SAMSum dataset.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n!pip install py7zr\n\ndataset_samsum = load_dataset(\"gigaword\")\nsplit_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\nprint(f\"\\nDialogue:\")\nprint(dataset_samsum[\"test\"][0][\"document\"])\nprint(\"\\nSummary\")\nprint(dataset_samsum[\"test\"][0][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:28:15.383675Z","iopub.execute_input":"2023-05-07T20:28:15.385639Z","iopub.status.idle":"2023-05-07T20:28:27.029166Z","shell.execute_reply.started":"2023-05-07T20:28:15.385591Z","shell.execute_reply":"2023-05-07T20:28:27.028205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (0.20.5)\nRequirement already satisfied: pycryptodomex>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from py7zr) (3.17)\nRequirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.2.3)\nRequirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.0)\nRequirement already satisfied: pyzstd>=0.14.4 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.15.7)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.6.7)\nRequirement already satisfied: pybcj>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.4)\nRequirement already satisfied: inflate64>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.3.1)\nRequirement already satisfied: brotli>=1.0.9 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11326a38558243f284ff5e511a77d26d"}},"metadata":{}},{"name":"stdout","text":"Split lengths: [3803957, 189651, 1951]\nFeatures: ['document', 'summary']\n\nDialogue:\njapan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .\n\nSummary\nnec UNK in computer sales tie-up\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Evaluate this using PEGASUS\npipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", framework='pt')\npipe_out = pipe(dataset_samsum[\"test\"][0][\"document\"])\nprint(\"Summary:\")\nprint(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:28:27.035447Z","iopub.execute_input":"2023-05-07T20:28:27.036091Z","iopub.status.idle":"2023-05-07T20:29:23.234143Z","shell.execute_reply.started":"2023-05-07T20:28:27.036059Z","shell.execute_reply":"2023-05-07T20:29:23.233085Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nYour max_length is set to 128, but you input_length is only 33. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n","output_type":"stream"},{"name":"stdout","text":"Summary:\nJapan 's nec corp. and UNK computer corp.<n>of the united states said they had agreed to join forces in supercomputer sales .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluating the entire test set\n\nWe will need a way to compare the baseline PEGASUS model to the finetuned version. We'll create an evaluation loop for this.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef chunks(list_of_elements, batch_size):\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\ndef evaluate_summaries(dataset, metric, model, tokenizer,\n                       batch_size=16, device=device,\n                       column_text=\"article\", column_summary=\"highlights\"):\n    article_batches = list(chunks(dataset[column_text], batch_size))\n    target_batches = list(chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n                        padding=\"max_length\", return_tensors=\"pt\")\n\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                                   attention_mask=inputs[\"attention_mask\"].to(device),\n                                   length_penalty=0.8, num_beams=8, max_length=128)\n\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n                                              clean_up_tokenization_spaces=True)\n                             for s in summaries]\n\n        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n        \n    return metric.compute(predictions=decoded_summaries, references=target_batch)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:29:23.235790Z","iopub.execute_input":"2023-05-07T20:29:23.236564Z","iopub.status.idle":"2023-05-07T20:29:23.313448Z","shell.execute_reply.started":"2023-05-07T20:29:23.236524Z","shell.execute_reply":"2023-05-07T20:29:23.312208Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load the model directly\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_ckpt = \"ainize/bart-base-cnn\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:29:23.318491Z","iopub.execute_input":"2023-05-07T20:29:23.318827Z","iopub.status.idle":"2023-05-07T20:29:31.096897Z","shell.execute_reply.started":"2023-05-07T20:29:23.318801Z","shell.execute_reply":"2023-05-07T20:29:31.095765Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate\nimport evaluate\n\n!pip install rouge_score\nrouge_metric = evaluate.load(\"rouge\")\nscore = evaluate_summaries(dataset_samsum[\"test\"], rouge_metric, model,\n                           tokenizer, column_text=\"document\",\n                           column_summary=\"summary\", batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:29:31.098404Z","iopub.execute_input":"2023-05-07T20:29:31.098779Z","iopub.status.idle":"2023-05-07T20:40:55.902105Z","shell.execute_reply.started":"2023-05-07T20:29:31.098743Z","shell.execute_reply":"2023-05-07T20:40:55.900972Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.4.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.28.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.64.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.13.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.11.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 244/244 [11:04<00:00,  2.72s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame(score, index=[\"bart\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:40:55.903665Z","iopub.execute_input":"2023-05-07T20:40:55.905235Z","iopub.status.idle":"2023-05-07T20:40:55.929853Z","shell.execute_reply.started":"2023-05-07T20:40:55.905182Z","shell.execute_reply":"2023-05-07T20:40:55.928891Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        rouge1    rouge2    rougeL  rougeLsum\nbart  0.235341  0.075865  0.211554   0.211612","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bart</th>\n      <td>0.235341</td>\n      <td>0.075865</td>\n      <td>0.211554</td>\n      <td>0.211612</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"In order to fine tune this model, we need to be able to tokenize the data. We can also limit the lengths of each dialogue and summary to 1024 and 128, respectively.","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch[\"document\"], truncation=True,\n                                max_length=1024)\n\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n                                     truncation=True)\n\n    return {\"input_ids\": input_encodings[\"input_ids\"],\n            \"attention_mask\": input_encodings[\"attention_mask\"],\n            \"labels\": target_encodings[\"input_ids\"]}\n\ndataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n                                       batched=True)\n\ncolumns = [\"input_ids\", \"labels\", \"attention_mask\"]\ndataset_samsum_pt.set_format(type=\"torch\", columns=columns)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:40:55.931414Z","iopub.execute_input":"2023-05-07T20:40:55.932009Z","iopub.status.idle":"2023-05-07T20:51:43.330453Z","shell.execute_reply.started":"2023-05-07T20:40:55.931972Z","shell.execute_reply":"2023-05-07T20:51:43.329584Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3804 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d2750dc9da45f78582acc1ce54d7b1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/190 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba1afbbdd2846ee868ab1548d9cec41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac346c31bb0488fb4bac22b37eba3f3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Preparing a batch of data\n\nWhen training `seq2seq` models, we need to apply \"teacher forcing\". The encoder will receive input tokens using the labels shifted by one as well as the encoder output. The prediction is then compared to the shifted labels to calculate the loss. To clarify, the decoder only sees the previous ground truth labels.\n\n`HuggingFace` provides a `DataCollatorForSeq2Seq` class that handles this for us.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:51:43.332327Z","iopub.execute_input":"2023-05-07T20:51:43.332724Z","iopub.status.idle":"2023-05-07T20:51:43.339278Z","shell.execute_reply.started":"2023-05-07T20:51:43.332689Z","shell.execute_reply":"2023-05-07T20:51:43.338312Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Gradient accumulation saves memory by updating the model only every X batches\ntraining_args = TrainingArguments(\n    output_dir=\"bart-samsum\", num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n    weight_decay=0.01, logging_steps=10, push_to_hub=False,\n    evaluation_strategy=\"steps\", eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:51:43.340897Z","iopub.execute_input":"2023-05-07T20:51:43.341232Z","iopub.status.idle":"2023-05-07T20:51:43.747576Z","shell.execute_reply.started":"2023-05-07T20:51:43.341201Z","shell.execute_reply":"2023-05-07T20:51:43.746679Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"##### Decreasing the size of `train` and `validation` from `3.8M, 189k to 30k, 3k` respectively. Because, when running with full dataset, it is taking too much to train the model, so I've decided to decrease the dataset size which can be completed under 4 hours. Also, I kept the `test` set as the same what dataset provides by default as it has only `1k rows`, so we can use the same as the test set.","metadata":{}},{"cell_type":"code","source":"train_sample = dataset_samsum_pt['train'].shuffle().select(range(50000))\nvalidation_sample = dataset_samsum_pt['validation'].shuffle().select(range(5000))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:51:43.749097Z","iopub.execute_input":"2023-05-07T20:51:43.749455Z","iopub.status.idle":"2023-05-07T20:51:44.963655Z","shell.execute_reply.started":"2023-05-07T20:51:43.749414Z","shell.execute_reply":"2023-05-07T20:51:44.962728Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install wandb --upgrade\ntrainer = Trainer(model=model, args=training_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_collator,\n                  train_dataset=train_sample,\n                  eval_dataset=validation_sample)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-07T20:51:44.965425Z","iopub.execute_input":"2023-05-07T20:51:44.965748Z","iopub.status.idle":"2023-05-07T22:29:20.184001Z","shell.execute_reply.started":"2023-05-07T20:51:44.965718Z","shell.execute_reply":"2023-05-07T22:29:20.182954Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.2)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.28.2)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.4)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.20.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteja-atech\u001b[0m (\u001b[33mspark5\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230507_205158-eec7bawc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/spark5/huggingface/runs/eec7bawc' target=\"_blank\">good-brook-9</a></strong> to <a href='https://wandb.ai/spark5/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/spark5/huggingface' target=\"_blank\">https://wandb.ai/spark5/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/spark5/huggingface/runs/eec7bawc' target=\"_blank\">https://wandb.ai/spark5/huggingface/runs/eec7bawc</a>"},"metadata":{}},{"name":"stderr","text":"You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1562' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1562/1562 1:36:42, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.425600</td>\n      <td>2.183717</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.196200</td>\n      <td>2.037916</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.350200</td>\n      <td>1.976657</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1562, training_loss=2.4293487020094475, metrics={'train_runtime': 5844.4964, 'train_samples_per_second': 8.555, 'train_steps_per_second': 0.267, 'total_flos': 1413587043348480.0, 'train_loss': 2.4293487020094475, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate after finetuning\nscore_ft = evaluate_summaries(\n    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n    batch_size=2, column_text=\"document\", column_summary=\"summary\")\npd.DataFrame(score_ft, index=[f\"bart_finetuned\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-07T22:29:20.189047Z","iopub.execute_input":"2023-05-07T22:29:20.190532Z","iopub.status.idle":"2023-05-07T22:34:50.822417Z","shell.execute_reply.started":"2023-05-07T22:29:20.190490Z","shell.execute_reply":"2023-05-07T22:34:50.821304Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 976/976 [05:30<00:00,  2.95it/s]\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                  rouge1    rouge2    rougeL  rougeLsum\nbart_finetuned  0.421053  0.235294  0.421053   0.421053","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bart_finetuned</th>\n      <td>0.421053</td>\n      <td>0.235294</td>\n      <td>0.421053</td>\n      <td>0.421053</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### Rouge score before and after training the bert-base-cnn. I've taken the dataset `Gigaword` from huggingface and limited number of rows to consider because it is such as big dataset which is taking lot of time to train.\nWe can see that there is increse in `ROUGE SCORE` before and after fine tuning the bart transformer.","metadata":{}},{"cell_type":"code","source":"sample_text = dataset_samsum[\"test\"][0][\"document\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n\ninputs = tokenizer(sample_text, max_length=1024, truncation=True,\n                   padding=\"max_length\", return_tensors=\"pt\")\n\nsummaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                           attention_mask=inputs[\"attention_mask\"].to(\n    device),\n    length_penalty=0.8, num_beams=8, max_length=128)\n\ndecoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n                                      clean_up_tokenization_spaces=True)\n                     for s in summaries]\n\ndecoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T22:34:50.830128Z","iopub.execute_input":"2023-05-07T22:34:50.833126Z","iopub.status.idle":"2023-05-07T22:34:51.126224Z","shell.execute_reply.started":"2023-05-07T22:34:50.833074Z","shell.execute_reply":"2023-05-07T22:34:51.125218Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(decoded_summaries)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T22:34:51.127631Z","iopub.execute_input":"2023-05-07T22:34:51.127946Z","iopub.status.idle":"2023-05-07T22:34:51.137148Z","shell.execute_reply.started":"2023-05-07T22:34:51.127915Z","shell.execute_reply":"2023-05-07T22:34:51.135780Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"['nepal UNK to join forces in supercomputer sales']\n","output_type":"stream"}]}]}